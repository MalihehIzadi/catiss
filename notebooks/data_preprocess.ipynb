{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2cb50276",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "import re\n",
    "import unicodedata as ud\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import words as w\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import os\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa5448a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data...\n",
      "(722899, 8) (80518, 8)\n"
     ]
    }
   ],
   "source": [
    "if not os.path.isfile(\"data/github-labels-top3-803k-train.csv\"):\n",
    "    print('downloading data...')\n",
    "    !cd data & curl \"https://tickettagger.blob.core.windows.net/datasets/github-labels-top3-803k-train.tar.gz\" | tar -xz\n",
    "\n",
    "if not os.path.isfile(\"data/github-labels-top3-803k-test.csv\"):\n",
    "    print('downloading data...')\n",
    "    !cd data & curl \"https://tickettagger.blob.core.windows.net/datasets/github-labels-top3-803k-test.tar.gz\" | tar -xz\n",
    "\n",
    "print('loading data...')\n",
    "train = pd.read_csv('data/github-labels-top3-803k-train.csv')\n",
    "test = pd.read_csv('data/github-labels-top3-803k-test.csv')\n",
    "print(train.shape, test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "206091c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "label= 'issue_label'\n",
    "time = 'issue_created_at'\n",
    "repo = 'repository_url'\n",
    "title  = 'issue_title'\n",
    "body = 'issue_body'\n",
    "author = 'issue_author_association'\n",
    "url = 'issue_url'\n",
    "label_col = 'labels'\n",
    "text_col = 'text'\n",
    "max_title = 30\n",
    "max_body = 170\n",
    "punctuations = '!\"$%&\\()*,/:;<=>[\\\\]^`{|}~+#@-`' #omitted ' for contractions, excluded # @ . ? -\n",
    "ascii_regex = re.compile(r'[^\\x00-\\x7f]')\n",
    "issue_regex = re.compile(r'#[0-9]+')\n",
    "function_regex = re.compile(r'[a-zA-Z][a-zA-Z0-9_.]*\\([a-zA-Z0-9_, ]*\\)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61081071",
   "metadata": {},
   "source": [
    "# first deduplicate the TRAINING dataset based on issue URls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "43edae1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of dropped issue duplications:  26220\n"
     ]
    }
   ],
   "source": [
    "dedup_train = train.sort_values(url).drop_duplicates(subset=[url]).copy()\n",
    "print('Number of dropped issue duplications: ' , train.shape[0] - dedup_train.shape[0])\n",
    "\n",
    "dedup_train[title] = dedup_train[title].astype(str)\n",
    "dedup_train[body] = dedup_train[body].astype(str)\n",
    "dedup_train[author] = dedup_train[author].astype(str)\n",
    "dedup_train[time] = dedup_train[time].astype(str)\n",
    "dedup_train[repo] = dedup_train[repo].astype(str)\n",
    "\n",
    "test[title] = test[title].astype(str)\n",
    "test[body] = test[body].astype(str)\n",
    "test[author] = test[author].astype(str)\n",
    "test[time] = test[time].astype(str)\n",
    "test[repo] = test[repo].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d2fa6f",
   "metadata": {},
   "source": [
    "# text normalization on text columns of issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "515c54d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Replacing functions...\n",
      "Replacing issue numbers...\n",
      "Converting to lower case...\n"
     ]
    }
   ],
   "source": [
    "print('Replacing functions...')\n",
    "dedup_train[body] = dedup_train[body].apply(lambda x:function_regex.sub(\" function \",x))\n",
    "test[body] = test[body].apply(lambda x:function_regex.sub(\" function \",x))\n",
    "\n",
    "print('Replacing issue numbers...')\n",
    "dedup_train[title] = dedup_train[title].apply(lambda x:issue_regex.sub(\" issue \",x))\n",
    "dedup_train[body] = dedup_train[body].apply(lambda x:issue_regex.sub(\" issue \",x))\n",
    "test[title] = test[title].apply(lambda x:issue_regex.sub(\" issue \",x))\n",
    "test[body] = test[body].apply(lambda x:issue_regex.sub(\" issue \",x))\n",
    "\n",
    "\n",
    "print('Converting to lower case...')\n",
    "dedup_train[title] = dedup_train[title].str.lower()\n",
    "dedup_train[body] = dedup_train[body].str.lower()\n",
    "test[title] = test[title].str.lower()\n",
    "test[body] = test[body].str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f10e06",
   "metadata": {},
   "source": [
    "# remove extra information from text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3edff6ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing punctuations...\n",
      "Removing non-ascii charachters...\n",
      "Replacing fixed part of repo URl column...\n",
      "Replacing white spaces...\n"
     ]
    }
   ],
   "source": [
    "print('Removing punctuations...')\n",
    "replace_string = ' '*len(punctuations)\n",
    "dedup_train[title] = dedup_train[title].str.translate(str.maketrans(punctuations, replace_string))\n",
    "dedup_train[body] = dedup_train[body].str.translate(str.maketrans(punctuations, replace_string))\n",
    "test[title] = test[title].str.translate(str.maketrans(punctuations, replace_string))\n",
    "test[body] = test[body].str.translate(str.maketrans(punctuations, replace_string))\n",
    "\n",
    "print('Removing non-ascii charachters...')\n",
    "dedup_train[title] = dedup_train[title].apply(lambda x:re.sub(ascii_regex, '', x))\n",
    "dedup_train[title] = dedup_train[title].apply(lambda x:ud.normalize('NFD', x))\n",
    "dedup_train[body] = dedup_train[body].apply(lambda x:re.sub(ascii_regex, '', x))\n",
    "dedup_train[body] = dedup_train[body].apply(lambda x:ud.normalize('NFD', x))\n",
    "\n",
    "test[title] = test[title].apply(lambda x:re.sub(ascii_regex, '', x))\n",
    "test[title] = test[title].apply(lambda x:ud.normalize('NFD', x))\n",
    "test[body] = test[body].apply(lambda x:re.sub(ascii_regex, '', x))\n",
    "test[body] = test[body].apply(lambda x:ud.normalize('NFD', x))\n",
    "\n",
    "print('Replacing fixed part of repo URl column...')\n",
    "dedup_train[repo] = dedup_train[repo].apply(lambda x: x.replace('https://api.github.com/repos/', ''))\n",
    "test[repo] = test[repo].apply(lambda x: x.replace('https://api.github.com/repos/', ''))\n",
    "\n",
    "print('Replacing white spaces...')\n",
    "dedup_train[title] = dedup_train[title].apply(lambda x:\" \".join(x.split()))\n",
    "dedup_train[body] = dedup_train[body].apply(lambda x:\" \".join(x.split()))\n",
    "test[title] = test[title].apply(lambda x:\" \".join(x.split()))\n",
    "test[body] = test[body].apply(lambda x:\" \".join(x.split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e6f6937",
   "metadata": {},
   "source": [
    "# truncate columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "238f3305",
   "metadata": {},
   "outputs": [],
   "source": [
    "dedup_train[title] = dedup_train[title].apply(lambda x: ' '.join(x.split(maxsplit=max_title)[:max_title]))\n",
    "dedup_train[body] = dedup_train[body].apply(lambda x: ' '.join(x.split(maxsplit=max_body)[:max_body]))\n",
    "test[title] = test[title].apply(lambda x: ' '.join(x.split(maxsplit=max_title)[:max_title]))\n",
    "test[body] = test[body].apply(lambda x: ' '.join(x.split(maxsplit=max_body)[:max_body]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c8a88cd",
   "metadata": {},
   "source": [
    "# prepare labels column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ee050430",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert categorical data to codes\n",
    "dedup_train[label] = pd.Categorical(dedup_train[label])\n",
    "test[label] = pd.Categorical(test[label])\n",
    "dedup_train[label_col] = dedup_train[label].cat.codes\n",
    "test[label_col] = test[label].cat.codes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f10c8480",
   "metadata": {},
   "source": [
    "# concat issue columns in one \"text\" column to feed the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0e70247b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# concat columns in a bag of sentences\n",
    "dedup_train[text_col] = 'time ' + dedup_train[time] + ' author ' + dedup_train[author] +' repo ' + dedup_train[repo] + ' title ' + dedup_train[title] + ' body ' + dedup_train[body]\n",
    "test[text_col] = 'time ' + test[time] + ' author ' + test[author] +' repo ' + test[repo] + ' title ' + test[title] + ' body ' + test[body]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dbf8829",
   "metadata": {},
   "source": [
    "#  split and save datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "17ffa7b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset index\n",
    "dedup_train.reset_index(drop=True, inplace=True)\n",
    "test.reset_index(drop=True, inplace=True)\n",
    "\n",
    "dedup_train[[text_col, label_col]].to_csv(f'data/train_clean_concat_{max_title+max_body}.csv', index = False)\n",
    "test[[text_col, label_col]].to_csv(f'data/test_clean_concat_{max_title+max_body}.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
